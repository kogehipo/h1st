{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3410832-bbea-43a1-9332-c72262b23623",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Modeler and Model with Iris dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ec153e-75c3-4f7d-9d49-25781e9909f9",
   "metadata": {},
   "source": [
    "次のtutorialを実行する。\n",
    "https://h1st.readthedocs.io/en/latest/tutorials/examples/modeler-model.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a165d92d-6a7f-4dc6-b552-e13c0c8452ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "このチュートリアルでは、例としてアイリス・データセットを使用して、ModelerとModelの作り方を学びます。\n",
    "\n",
    "まず、アイリス・データセットをロードするために、load_dataメソッドを持つMLModelerを作ります。そして、LogisticRegressionを学習するために学習データを作ります。H1stフレームワークはload_dataをコールして、あなたが定義した対応するMLModelを訓練するためのメソッドをビルドします。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9626b52-9f86-4999-8e7a-109fd8649e73",
   "metadata": {},
   "source": [
    "This tutorial shows how you can create Modeler and Model using the iris dataset as an example. Firstly, let’s create an MLModeler with load_data to load the iris dataset and generate training data to train a LogisticRegression base model in train. The h1st framework provides the build method which calls load_data and train and produces the corresponding MLModel which you needs to define."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2768d1e-8bfd-4925-a88a-1bceb919d5cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Any, Dict\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from h1st.model.model import Model\n",
    "from h1st.model.ml_modeler import MLModeler\n",
    "from h1st.model.ml_model import MLModel\n",
    "\n",
    "class MyMLModeler(MLModeler):\n",
    "    def __init__(self):\n",
    "        self.stats = {}\n",
    "        self.example_test_data_ratio = 0.2\n",
    "\n",
    "    def load_data(self) -> Dict:\n",
    "        df_raw = datasets.load_iris(as_frame=True).frame\n",
    "        return self.generate_training_data({'df_raw': df_raw})\n",
    "\n",
    "    def preprocess(self, data):\n",
    "        self.stats['scaler'] = StandardScaler()\n",
    "        return self.stats['scaler'].fit_transform(data)\n",
    "\n",
    "    def generate_training_data(self, data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        df_raw = data['df_raw']\n",
    "        df_raw.columns = ['sepal_length','sepal_width','petal_length','petal_width', 'species']\n",
    "\n",
    "        self.stats['targets'] = ['Setosa', 'Versicolour', 'Virginica']\n",
    "        self.stats['targets_dict'] = {k: v for v, k in enumerate(self.stats['targets'])}\n",
    "\n",
    "        # Shuffle all the df_raw\n",
    "        df_raw = df_raw.sample(frac=1, random_state=5).reset_index(drop=True)\n",
    "\n",
    "        # Preprocess data\n",
    "        df_raw.loc[:, 'sepal_length':'petal_width'] = self.preprocess(\n",
    "            df_raw.loc[:, 'sepal_length':'petal_width'])\n",
    "\n",
    "        # Split to training and testing data\n",
    "        n = df_raw.shape[0]\n",
    "        n_test = int(n * self.example_test_data_ratio)\n",
    "        training_data = df_raw.iloc[n_test:, :].reset_index(drop=True)\n",
    "        test_data = df_raw.iloc[:n_test, :].reset_index(drop=True)\n",
    "\n",
    "        # Split the data to features and labels\n",
    "        train_data_x = training_data.loc[:, 'sepal_length':'petal_width']\n",
    "        train_data_y = training_data['species']\n",
    "        test_data_x = test_data.loc[:, 'sepal_length':'petal_width']\n",
    "        test_data_y = test_data['species']\n",
    "\n",
    "        # When returning many variables, it is a good practice to give them names:\n",
    "        return {\n",
    "            'train_x':train_data_x,\n",
    "            'train_y':train_data_y,\n",
    "            'test_x':test_data_x,\n",
    "            'test_y':test_data_y,\n",
    "        }\n",
    "\n",
    "    def train_base_model(self, data: Dict[str, Any]) -> Any:\n",
    "        X, y = data['train_x'], data['train_y']\n",
    "        model = LogisticRegression(random_state=0)\n",
    "        model.fit(X, y)\n",
    "        return model\n",
    "\n",
    "    def evaluate_model(self, data: Dict, model: MLModel) -> Dict:\n",
    "        super().evaluate_model(data, model)\n",
    "        X, y_true = data['test_x'], data['test_y']\n",
    "        y_pred = pd.Series(model.predict({'X': X, 'y': y_true})['species']).map(model.stats['targets_dict'])\n",
    "        return {'micro_f1_score': f1_score(y_true, y_pred, average='micro')}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70cc49b2-80dd-4ea5-bf3d-7719c83a3b0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MyMLModel(MLModel):\n",
    "    def preprocess(self, data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        raw_data = data['X']\n",
    "        return {\n",
    "            'X': self.stats['scaler'].transform(raw_data)\n",
    "        }\n",
    "\n",
    "    def predict(self, input_data: dict) -> dict:\n",
    "        preprocess_data = self.preprocess(input_data)\n",
    "        y = self.base_model.predict(preprocess_data['X'])\n",
    "        return {'species': [self.stats['targets'][item] for item in y]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc1957a6-8313-4aa1-9c8c-627e4c4b66a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'micro_f1_score': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n"
     ]
    }
   ],
   "source": [
    "#import pandas as pd\n",
    "#from IrisModel import MyMLModeler\n",
    "#from IrisModel import MyMLModel\n",
    "\n",
    "my_ml_modeler = MyMLModeler()\n",
    "my_ml_modeler.model_class = MyMLModel\n",
    "\n",
    "my_ml_model = my_ml_modeler.build_model()\n",
    "\n",
    "print(my_ml_model.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7538f287-182b-4036-8470-14517bc4b66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'species': ['Setosa', 'Versicolour']}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = my_ml_model.predict({\n",
    "    'X': pd.DataFrame(\n",
    "        [[5.1, 3.5, 1.5, 0.2],\n",
    "        [7.1, 3.5, 1.5, 0.6]],\n",
    "        columns=['sepal_length','sepal_width','petal_length','petal_width'])\n",
    "})\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a928c017-d718-421e-8c5e-8c9b51fca178",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
